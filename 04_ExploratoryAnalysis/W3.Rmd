Week 3. Exploratory Data Analysis
=================================
Hierarchical Clustering
------------------------
Define close:
* continuous - euclidean distance
* continuous - correlation simalirity
* binary - manhattan distance
```{r}
set.seed(1234)
old.par = par()
par(mar = c(0,0,0,0))
x = rnorm(12, mean = rep(1:3, each=4), sd=0.2)
y = rnorm(12, mean = rep(c(1,2,1), each=4), sd = 0.2)
plot(x, y, col='blue', pch=10, cex=2)
text(x+0.05, y+0.05, labels=as.character(1:12))

dataFrame = data.frame(x=x,y=y)
# dist() with different methods
distxy = dist(dataFrame, method='euclidean')
hClustering = hclust(distxy)
plot(hClustering)
# draw horizontal line to cut trees
# to decide # of clusters i want
# y axis shows 'height', which is the distance
# measurement during clustering. Its method can be
# centroid, average,complete etc
abline(h=1.5, lty=2, col='red')
```

Prettier dendrograms: a display of hierarchical cluster with coloured leaf labels
```{r}
myplclust <- function(hclust, lab = hclust$labels, lab.col = rep(1, length(hclust$labels)), 
                      hang = 0.1, ...){
  y = rep(hclust$height, 2)
  x = as.numeric(hclust$merge)
  y = y[which(x < 0)]
  x = x[which(x < 0)]
  x = abs(x)
  y = y[order(x)]
  x = x[order(x)]
  plot(hclust, labels=F, hang=hang,...)
  text(x=x,y=y[hclust$order]-(max(hclust$height)*hang), labels=lab[hclust$order],
       col=lab.col[hclust$order], srt=90, adj=c(1,0.5),xpd=NA,...)
}
myplclust(hClustering, lab=rep(1:3, each=4), lab.col=rep(1:3, each=4))
```

useful function heatmap()
```{r}
set.seed(143)
dataMatrix = as.matrix(dataFrame)[sample(1:12),]
# heatmap with heatmap(), also giving dendogram on both xy axes
heatmap(dataMatrix)
```

The picture may be unstable
* change a few points
* have different missing values
* pick different distance
* change the merging strategy
* change the scale of points for one variable
but it is deterministic
should be primarily used for exploration

K-Means Clustering
-------------------
```{r}
kmeansObg = kmeans(dataFrame, centers=3)
names(kmeansObg)
par(mar = rep(0.2,4))
plot(x,y,col=kmeansObg$cluster, pch=19, cex=2)
points(kmeansObg$centers, col=1:3, pch=3, cex=3, lwd=3)

# heatmap with image()
par(mfrow=c(1,2), mar=c(2,4,0.1,0.1))
image(t(dataMatrix)[,nrow(dataMatrix):1], yaxt='n')
image(t(dataMatrix)[,order(kmeansObg$cluster)], yaxt='n')
```

kmeans is not deterministic
* different # of clusters
* different # of iterations

Dimension Reduction
--------------------
### PCA and SVD
```{r}
set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix = matrix(rnorm(400), nrow=40)
# messy not quite useful
image(1:10, 1:40, t(dataMatrix)[,nrow(dataMatrix):1])
# cluster the data
heatmap(dataMatrix)

# add a pattern to the data set
set.seed(6789)
for (i in 1:40){
  # flip a coin
  coinFlip = rbinom(1, size = 1, prob = 0.5)
  # if coin is heads, add a common pattern to that row
  if (coinFlip){
    dataMatrix[i,] = dataMatrix[i,] + rep(c(0,3), each = 5)
  }
}

heatmap(dataMatrix)
image(1:10, 1:40, t(dataMatrix)[,nrow(dataMatrix):1])
```

Patterns in rows and columns
```{r}
# hclust combined with colmeans and rowmeans
hh = hclust(dist(dataMatrix))
dataMatrixOrdered = dataMatrix[hh$order,]
par(old.par)
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, xlab='row mean', ylab='row', pch=19)
plot(colMeans(dataMatrixOrdered), xlab='column',ylab='column mean', pch=19)
```

Components of SVD - u and v
```{r}
svd1 = svd(scale(dataMatrixOrdered))
par(old.par)
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd1$u[,1],40:1, xlab='row',ylab='first left singular vector', pch=19)
plot(svd1$v[,1], xlab='column', ylab='first right singular vector', pch=19)
```

Components of SVD - variance explained
```{r}
par(old.par)
par(mfrow=c(1,2))
plot(svd1$d, xlab='column', ylab='singular value', pch=19)
plot(svd1$d^2/sum(svd1$d^2), xlab='column', ylab='prop. of variance explained', pch=19)
```

Relationship to principal components
```{r}
svd1 = svd(scale(dataMatrixOrdered))
pca1 = prcomp(dataMatrixOrdered, scale=T)
par(old.par)
plot(pca1$rotation[,1], svd1$v[,1], pch=19, xlab='principal component 1', ylab='right singular vector 1')
abline(c(0,1))
```

Missing values handling
```{r}
library(impute)  # not available for r 3.1.0
dataMatrix2 = dataMatrixOrdered
dataMatrix2[sample(1:100,size=40,replace=F)] = NA
dim(dataMatrix2)
dataMatrix2 = impute.knn(dataMatrix2)$data
svd1 = svd(scale(dataMatrixOrdered))  # full complete data
svd2 = svd(scale(dataMatrix2))        # na imputed data
par(old.par)
par(mfrow=c(1,2))
plot(svd1$v[,1], pch=19)
plot(svd2$v[,1], pch=19)
```

Face example
```{r}
load('./dimensionReduction/data/face.rda')
image(t(faceData)[, nrow(faceData):1])
svd1 = svd(scale(faceData))
plot(svd1$d^2/sum(svd1$d^2), xlab='column', ylab='prop. of variance explained', pch=19)

# subset of svd to approximate orginal image
approx1 = svd1$u[,1] %*% t(svd1$v[,1]) * svd1$d[1]
approx5 = svd1$u[,1:5] %*% diag(svd1$d[1:5]) %*% t(svd1$v[,1:5])
approx10 = svd1$u[,1:10] %*% diag(svd1$d[1:10]) %*% t(svd1$v[,1:10])

par(mfrow=c(1,4))
image(t(approx1)[,nrow(approx1):1], main='(a)')
image(t(approx5)[,nrow(approx5):1], main='(b)')
image(t(approx10)[,nrow(approx10):1], main='(c)')
image(t(faceData)[,nrow(faceData):1], main='(d)')

```

Notes:
* scale matters
* PC's / SV's may mix real patterns
* can be computationally intensive
Alternatives:
* factor analysis
* independent component analysis
* latent semantic analysis